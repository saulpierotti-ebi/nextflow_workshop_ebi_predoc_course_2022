# Basic features

## Multiple processes and publication of workflow outputs

Let's now step up a bit the complexity and write a workflow containing two processes that communicate with each other.
Add the following process to your `main.nf` file.

```groovy
process duplicate_lines {
    publishDir "../nextflow_output/duplicate_lines"

    input:
        path my_file
    output:
        path "${my_file.simpleName}.duplicated.txt"
    script:
        """
        cat $my_file $my_file > ${my_file.simpleName}.duplicated.txt
        """
}
```

And modify your workflow block as follows

```groovy
workflow {
    say_hello()
    duplicate_lines( say_hello.out )
}
```

If you want to see how your `main.nf` should look like at this stage open this hidden section


<details>
<summary>Code</summary>
```groovy
nextflow.enable.dsl = 2

// this is a comment
process say_hello {
    // comments can be written anywhere
    output:
        path "hello_world.txt"
    script:
        """
        echo "This is the EBI predoc course" > hello_world.txt
        """
}

process duplicate_lines {
    publishDir "../nextflow_output/duplicate_lines"

    input:
        path my_file
    output:
        path "${my_file.simpleName}.duplicated.txt"
    script:
        """
        cat $my_file $my_file > ${my_file.simpleName}.duplicated.txt
        """
}

workflow {
    say_hello()
    duplicate_lines( say_hello.out )
}
```
</details>

Now run again the workflow with `nextflow run main.nf`.
This time your workflow will not print anything since we omitted the `view()` operator.
However, if you type in your terminal the following

```bash
cat ../nextflow_output/duplicate_lines/hello_world.duplicated.txt
```

You should see the following

```
This is the EBI predoc course
This is the EBI predoc course
```

So as you see, the content of the `hello_world.txt` file that we created before has been duplicated so that it appears twice and it has been placed on a new file, `../nextflow_output/duplicate_lines/hello_world.duplicated.txt`.

Let's analyse what happened this time:

- We created another process called `duplicate_lines` which duplicates a given file thanks to its `script` command `cat $my_file $my_file > ${my_file.simpleName}.duplicated.txt`
- `duplicate_lines` declares an `input:` block. The input block is similar to the output block that we saw before in that it can use the `path` qualifier to declare the input to be a file. However, what comes after `path` does not need to be a real filename, it is just a variable name (note that it is not quoted). Nextflow replaces that variable with the real name of the file given in input.
- In the script and output blocks we can refer to the inputs by specifying `$my_file`. Note that we need to use the same name used in the input declaration.
  - It is possible to enclose the variable name in curly braces to demark it from other text. So `${my_file}` is equivalent to `$my_file`.
  - We applied the operator `simpleName` to the variable `$my_file` by writing `${my_file.simpleName}`. This removes the path and the extension from `my_file`, so that we can use it to name our output file (if `$my_file` contains the value `"/some/path/hello_world.txt"`, then `${my_file.simpleName}` contains only `hello_world`).
- We used a new directive in the process `duplicate_lines`, called `publishDir`. This specifies the folder where the output of the process should be placed at the end of the execution. This is usually done to put the final outputs of your workflow in a meaningful directory structure. In our case, `publishDir "../nextflow_output/duplicate_lines"` places `${my_file.simpleName}.duplicated.txt` in the folder `../nextflow_output/duplicate_lines`
- In the workflow block we called the process `duplicate_lines` with `say_hello.out` as an argument. So the output of `say_hello` is used as an input for `duplicate_lines`. Note that the number of arguments provided to a process must match the number of arguments declared in its input block.

## Creating channels and specifying pipeline parameters

The workflow that we wrote works as expected but it is not very useful since we cannot specify dynamically its input files. We will now learn how to use pipeline parameters to specify an external input file for the whole workflow and how to use channel factories to feed it to our processes.

First create a text file called `../nextflow_inputs/a_file.txt` containing the string `"This is the file content"`

```bash
mkdir ../nextflow_inputs
echo "This is the file content" > ../nextflow_inputs/a_file.txt
```

Modify the workflow as follows

```groovy
workflow {
    Channel.fromPath( params.input_file ).set{ input_ch }
    duplicate_lines( input_ch )
}
```

If you want to see how your `main.nf` should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
nextflow.enable.dsl = 2

// this is a comment
process say_hello {
    // comments can be written anywhere
    output:
        path "hello_world.txt"
    script:
        """
        echo "This is the EBI predoc course" > hello_world.txt
        """
}

process duplicate_lines {
    publishDir "../nextflow_output/duplicate_lines"

    input:
        path my_file
    output:
        path "${my_file.simpleName}.duplicated.txt"
    script:
        """
        cat $my_file $my_file > ${my_file.simpleName}.duplicated.txt
        """
}

workflow {
    Channel.fromPath( params.input_file ).set{ input_ch }
    duplicate_lines( input_ch )
}
```
</details>

Now open the file `nextflow.config` and add the following

```groovy
params {
  input_file = "../nextflow_inputs/a_file.txt"
}
```

If you want to see how your `nextflow.config` should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
// you can also put comments in nextflow.config
workDir = "../nextflow_workdir"

params {
  input_file = "../nextflow_inputs/a_file.txt"
}
```
</details>

Now run again the workflow with `nextflow run main.nf`.
This time if you examine the folder `../nextflow_output/duplicate_lines` you will find a file called `a_file.duplicated.txt`. Let's see it's content.

```bash
cat ../nextflow_output/duplicate_lines/a_file.duplicated.txt
```

This should print

```
This is the file content
This is the file content
```

So the content of the file that we created before, `a_file.txt` was duplicated and placed in the file `a_file.duplicated.txt` in the folder `../nextflow_output/duplicate_lines`.
Let's examine what happened:

- In `nextflow.config`, we declared a `params` block. Any variable written inside this block, like `input_file = "../nextflow_inputs/a_file.txt"`, is accessible in the workflow by writing `params.<variable_name>`, so `params.input_file` in this case.
- In `main.nf`, we wrote `Channel.fromPath( params.input_file )`. This uses the channel factory `Channel.fromPath` to create a new channel using the content of `params.input_file`. The `Channel.fromPath` factory interprets its argument to be the path to a file.
- After `Channel.fromPath( params.input_file )` we added `set{ input_ch }` (note the curly braces: it is a [closure](https://www.wikiwand.com/en/Closure_(computer_programming)), we will explore later what does it mean). This assigns to the newly created channel the name `input_ch`
- We then used the channel `input_ch` as an input for the process `duplicate_lines` in the statement `duplicate_lines( input_ch )`. Since the channel `input_ch` contains the variable `params.input_file`, which we declared to contain the value `../nextflow_inputs/a_file.txt`, this file is given in input to `duplicate_lines`
- `duplicate_lines` performed its function as before, putting its output in `../nextflow_output/duplicate_lines`

Instead than hard-coding parameters in `nextflow.config`, it is also possible to pass them on the command line. So for example we could have omitted the `params` block in `nextflow.config` and run

```bash
nextflow run main.nf --input_file ../nextflow_inputs/a_file.txt
```

This produces the same result. Note that pipeline parameters need to be specified with two prepending dashes (`--my_parameter`).
This differentiates them from command line options for Nextflow itself, such as `-dry-run`, which use a single dash.

## Multiple input files

We are now able to process a single file with our pipeline, but what if we have many files that we need to process in the same way?
One approach for this is to use a [glob pattern](https://www.wikiwand.com/en/Glob_(programming)) as an input.

Let's create a bunch of files to use in input

```bash
echo "This is content of file 1" > ../nextflow_inputs/set_of_files_1.txt
echo "This is content of file 2" > ../nextflow_inputs/set_of_files_2.txt
echo "This is content of file 3" > ../nextflow_inputs/set_of_files_3.txt
echo "This is content of file 4" > ../nextflow_inputs/set_of_files_4.txt
echo "This is content of file 5" > ../nextflow_inputs/set_of_files_5.txt
echo "This is content of file 6" > ../nextflow_inputs/set_of_files_6.txt
echo "This is content of file 7" > ../nextflow_inputs/set_of_files_7.txt
echo "This is content of file 8" > ../nextflow_inputs/set_of_files_8.txt
```

Now we just need to modify `input_file` in `nextflow.config`

```groovy
params {
  input_file = "../nextflow_inputs/set_of_files_*.txt"
}
```

If you want to see how your `nextflow.config` should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
// you can also put comments in nextflow.config
workDir = "../nextflow_workdir"

params {
  input_file = "../nextflow_inputs/set_of_files_*.txt"
}
```
</details>

Now run again the workflow with `nextflow run main.nf`.
This time if you examine the output folder with `ls ../nextflow_output/duplicate_lines` you will find a set of files

```
a_file.duplicated.txt
set_of_files_1.duplicated.txt
set_of_files_2.duplicated.txt
set_of_files_3.duplicated.txt
set_of_files_4.duplicated.txt
set_of_files_5.duplicated.txt
set_of_files_6.duplicated.txt
set_of_files_7.duplicated.txt
set_of_files_8.duplicated.txt
```

Like before, each of them will contain the duplicated version of the original files.

Let's examine what happened:

- We changed `params.input_ch` to contain a glob pattern. This is expanded by Nextflow to yield a list of matching files.
- The matching files are fed one by one via the channel `input_ch` to the process `duplicate_lines`
- `duplicate_lines` operates independently but in parallel on all the inputs, producing the output files. Each task is executed in its own private directory.

## Using a sample sheet and value channels

Using glob patterns for specifying samples is useful and quick, but what if we want to specify input files that live in many different directories with very different names?
And if we want some files to be processed in pairs with other specific files, or with specific parameters?
For such use cases a sample sheet is the easiest solution and it is the recommended way to specify workflow inputs.

A sample sheet is just a [csv](https://www.wikiwand.com/en/Comma-separated_values) file with one row per file to be processed, and with each column specifying either a file or a parameter.
Create a sample sheet called `../nextflow_inputs/samplesheet.csv` by running the following command.


```bash
echo "sample,content" > ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_1.txt,csv_content_1" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_2.txt,csv_content_2" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_3.txt,csv_content_3" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_4.txt,csv_content_4" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_5.txt,csv_content_5" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_6.txt,csv_content_6" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_7.txt,csv_content_7" >> ../nextflow_inputs/samplesheet.csv
echo "$(pwd)/../nextflow_inputs/set_of_files_8.txt,csv_content_8" >> ../nextflow_inputs/samplesheet.csv
```

Now create a process called `append_to_file`

```groovy
process append_to_file {
    publishDir "../nextflow_output/append_to_file"

    input:
        path my_file
        val my_val
    output:
        path "${my_file.simpleName}.appended.txt"
    script:
        """
        cat $my_file > ${my_file.simpleName}.appended.txt
        echo "$my_val" >> ${my_file.simpleName}.appended.txt
        """
}
```

And modify the workflow as follows

```groovy
workflow {
    Channel.fromPath( params.samplesheet )
      .splitCsv( header: true )
      .set{ input_ch }

    input_ch.map{ it["sample"] }.set{ sample_ch }
    input_ch.map{ it["content"] }.set{ content_ch }

    duplicate_lines( sample_ch )
    append_to_file( duplicate_lines.out, content_ch )
}
```

If you want to see how your `main.nf` file should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
nextflow.enable.dsl = 2

// this is a comment
process say_hello {
    // comments can be written anywhere
    output:
        path "hello_world.txt"
    script:
        """
        echo "This is the EBI predoc course" > hello_world.txt
        """
}

process duplicate_lines {
    publishDir "../nextflow_output/duplicate_lines"

    input:
        path my_file
    output:
        path "${my_file.simpleName}.appended.txt"
    script:
        """
        cat $my_file $my_file > ${my_file.simpleName}.duplicated.txt
        """
}

process append_to_file {
    publishDir "../nextflow_output/append_to_file"

    input:
        path my_file
        val my_val
    output:
        path "${my_file.simpleName}.duplicated.txt"
    script:
        """
        cat $my_file > ${my_file.simpleName}.appended.txt
        echo "$my_val" >> ${my_file.simpleName}.appended.txt
        """
}

workflow {
    Channel.fromPath( params.samplesheet )
      .splitCsv( header: true )
      .set{ input_ch }

    input_ch.map{ it["sample"] }.set{ sample_ch }
    input_ch.map{ it["content"] }.set{ content_ch }

    duplicate_lines( sample_ch )
    append_to_file( duplicate_lines.out, content_ch )
}
```
</details>

Now we need to modify the `params` block in your `nextflow.config`

```groovy
params {
  samplesheet = "../nextflow_inputs/samplesheet.csv"
}
```

If you want to see how your `nextflow.config` should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
// you can also put comments in nextflow.config
workDir = "../nextflow_workdir"

params {
  samplesheet = "../nextflow_inputs/samplesheet.csv"
}
```
</details>

Now run again the workflow with `nextflow run main.nf`.
This time if you examine the output folder with `ls ../nextflow_output/append_to_file` you will find a set of files

```
set_of_files_1.append_to_file.txt
set_of_files_2.append_to_file.txt
set_of_files_3.append_to_file.txt
set_of_files_4.append_to_file.txt
set_of_files_5.append_to_file.txt
set_of_files_6.append_to_file.txt
set_of_files_7.append_to_file.txt
set_of_files_8.append_to_file.txt
```

Taking as an example the first one, `set_of_files_1.append_to_file.txt` we would expect it to contain

```
This is content of file 1
This is content of file 1
csv_content_1
```

However, most probably it will contain something like

```
This is content of file 1
This is content of file 1
csv_content_3
```

So the first two lines have the correct number but the last line shows the wrong element! What's more, the element shown on the last line can be different on each run (try!). We said before that channels are guaranteed to emit elements in the same order that they are received, so we should have the correct matches between files and values. What happened?

What happened is that while channels are indeed guaranteed to preserve order, processes and some operators are not because of their asynchronous and parallel nature. In our case, the process `duplicate_lines` receives the files in the correct order, but it is not guaranteed to emit them in the same order. This is because all the file are processed in parallel, and for example `set_of_files_5.append_to_file.txt` could be processed and emitted some milliseconds earlier that `set_of_files_1.append_to_file.txt`, and so the order would be altered.

A better approach when we want multiple channels to be used by one process with some predictable matching is to join channels using matching keys. We will explore it in a later section.
For now, let's examine what happened in this version of our workflow:

- We created the process `append_to_file`, which takes in input a file (`path my_file`) and a value (`val my_val`). This process copies the content of `my_file` to `${my_file.simpleName}.append_to_file.txt` and then appends to the newly created file the content of `my_val`
- We defined the parameter `samplesheet` in `nextflow.config`
- We modified the workflow so that we use `params.samplesheet` to create a channel with `Channel.fromPath`
- We applied the operator `splitCsv` to this channel containing the samplesheet, using the option `header: true`. This operator reads the file contained in the channel assuming it to be a csv file with an header line. Then it produces in output another channel containing, for each row in the samplesheet, a groovy map (what in Python would be called a [dictionary](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_1/Fundamentals_of_data_structures/Dictionaries)). This map contains a key for each element in the csv header, and a value corresponding to the value of that column in the current line of the csv file.
- We apply the `map` operator (`.map{ it["sample"] }`) to the resulting channel
  - The map operator is follwed by a [closure](https://www.wikiwand.com/en/Closure_(computer_programming)) (`{ it["sample"] }`)
  - You can think of a closure as an unnamed function (like a lambda function in Python)
  - The `map` operator calls this unnamed function for each element in the channel, and creates another channel containing the respective function outputs
  - Closure understand the implicit variable `it`, which represents whatever imput is given to the function
  - If an explicit return statement is not present in the closure, the last evaluated expression is returned by the closure ( `it["sample"]` in this case)
- Since `splitCsv` created a dictionary for each item in the samplesheet, `map{ it["sample"] }` replaces that dictionary (`it` in the closure) with the content of the value contained in it under the key `"sample"`
- `input_ch` is used again in `input_ch.map{ it["content"] }.set{ content_ch }` to extract the value of `content` for each sample from the samplesheet
  - Channels can be used many times, and each time all the files contained in it are processed. So both `input_ch.map{ it["content"] }.set{ content_ch }` and `input_ch.map{ it["sample"] }.set{ sample_ch }` process every element in `input_ch`
- We created the channels `sample_ch` and `content_ch` using the `set` operator. They contain respectively the sample files and the `content` values related to them.
  - The order of processing in a channel is guaranteed, so we can be sure that each sample will be paired with the correct `content` value
- We fed `sample_ch` to `duplicate_lines`
- We fed the output of `duplicate_lines` to `append_to_file`, together with the values in `content_ch`
- `append_to_file` appended the value of `content_ch` to the output of `duplicate_lines`, creating the final result

Note that we introduced a new qualifier: `val`. The `val` qualifier specifies a value, differently from the `path` qualifier that specifies a file. If I write `val variable_name`, then `variable_name` can contain something like a string or a number, depending on what is fed to the process input.
It is also possible to manually create value channels using the channel factory `Channel.value`.
Take as an example the following

```groovy
workflow{
  Channel.value(1, 2, 3).view()
}
```

This would produce a channel containing in order the values 1, 2, and 3. These would then be printed one by one to the terminal by the view operator.

## Software dependencies

We can do many things with just shell scripts, but for real-word scenarios we would probably need to use some additional library or tool.
Software dependencies for a process can be specified in Nextflow in two main ways: using [software containers](https://www.docker.com/resources/what-container/) or [virtual environments](https://realpython.com/python-virtual-environments-a-primer/).

A software container is like a shipping container: it is a monolithic block of code and data that contains everything needed to run a specific application, including the operating system (but not the kernel, the host kernel is used). The most popular container engine (the software that is used to actually run a container) is [Docker](https://www.docker.com/). In high-performance clusters, for security reasons Docker is often discouraged or forbidden. A popular alternative container engine used in bioinformatics is [Singularity](https://sylabs.io/singularity/). The good news is that Singularity is also able to run containers created with Docker, so we can consider them equivalent for our purpose.

For popular tools, a container has most probably already been created by someone and put in a public container registry like [DockerHub](https://hub.docker.com/). In this case, we can provide to Nextflow just a link to the container and it will do the rest.
If instead we need to use a custom container, we would need to write an appropriate description file for it (called a "Dockerfile"), use it to actually build the container, and then upload the container to DockerHub.
This last use-case will not be treated in this workshop, and we will limit ourselves to the use of pre-built containers.

On the other hand, virtual environments are a more lightweight and modifiable alternative to containers.
They rely on the host operating system but they allow to define additional software.
Nextflow is compatible with [Conda](https://docs.conda.io/en/latest/) environments, and can also use the faster [Mamba](https://mamba.readthedocs.io/en/latest/index.html) implementation of Conda.
It is worth noting that while Conda/Mamba are typically thought of as package managers for Python, they are actually able to install also R packages, the R interpreter itself, stand-alone tools, and even GPU drivers.

Differently from containers, environments do not need to be pre-built to be used by Nextflow.
It is possible to just define the software that we need and Nextflow will take care to build an appropriate environment with it.

Let's now write a workflow that makes use of virtual environments.

First download a csv file containing a set of countries, their population, and their area by running the following

```bash
wget https://raw.githubusercontent.com/saulpierotti-ebi/saulpierotti-ebi.github.io/main/assets/population_area_by_country.csv
```

Now modify your `main.nf` file and make it look like this

```groovy
nextflow.enable.dsl = 2

process calculate_population_density {
    conda "python=3.10.6 pandas=1.5.0"

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.with_density.csv"
    script:
        """
        #!/usr/bin/env Python

        import pandas

        df = pandas.read_csv("${csv_file}")
        df["density"] = df["population"] / df["area"]
        df.to_csv("${csv_file.simpleName}.with_density.csv")
        """
}

process plot_population_by_area {
    publishDir "../nextflow_output/plots"

    conda "r-base=4.1.3 r-tidyverse=1.3.2"

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.pdf"
    script:
        """
        #!/usr/bin/env Rscript

        library("tidyverse")

        df <- read_csv("${csv_file}")

        ggplot(df, aes(x=area, y=population, label=country, size=density)) +
            geom_text()

        ggsave("${csv_file.simpleName}.pdf")
        """
}

workflow {
    input_ch = Channel.fromPath( params.input )
    calculate_population_density( input_ch )
    plot_population_by_area( calculate_population_density.out )
}
```

Modify your `nextflow.config` by adding the following

```groovy
conda {
    // use the faster mamba solver instead of conda
    useMamba = true
}
```
If you want to see how your `nextflow.config` should look like at this stage open this hidden section

<details>
<summary>Code</summary>
```groovy
// you can also put comments in nextflow.config
workDir = "../nextflow_workdir"

params {
  samplesheet = "../nextflow_inputs/samplesheet.csv"
}

conda {
    // use the faster mamba solver instead of conda
    useMamba = true
}
```
</details>

Now run the workflow as follows

```bash
nextflow run main.nf --input population_area_by_country.csv
```

This time the workflow will take significantly longer to run since Nextflow needs to create two mamba environments.
However, this will affect only the first run of the workflow, since once an environment is created it can be used multiple times.
Now open the file `../nextflow_output/plots/population_area_by_country.pdf`.
It will contain a plot of the population of different countries versus their area, with the size proportional to their population density.

Let's explore what happened:

  - We specified to Nextflow to use mamba instead than conda when using the conda directive (with `conda{use.mamba = true}` in `nextflow.config`)
  - As before, we used a command-line flag to pass an input file to the workflow (`--input population_area_by_country.csv`)
  - We used `population_area_by_country.csv` as an input for the process `calculate_population_density`, which uses Python code to calculate the population density of a country from its area and total population
  - The output of `calculate_population_density` is fed to the process `plot_population_by_area`, which uses R code to produce the plot

The `conda` directive used in these processes defines the software dependencies needed, in this case Python and [pandas](https://pandas.pydata.org/) for the first process and R and [tidyverse](https://www.tidyverse.org/) for the second process.
Software versions can be specified with an equal sign after the software name.
Packages are sourced from the repository [Anaconda](https://anaconda.org/).

Another thing to note here: the `script` directive normally interprets the code written in it as bash code, but if an sh-bang (`!#`) is included in the first line of the script, then the software specified in the sh-bang is used to run the script section (so `!#/usr/bin/env Python` tells to Nextflow to interpret the script section as Python code).

A different approach to reach the same goal is to write a separate Python or R script, place it in the `bin` folder, and then execute it with a bash command in the script directive.

So for example you can create a file called `bin/calculate_population_density.py` with the following content

```python
import pandas
import sys

df = pandas.read_csv(sys.argv[1])
df["density"] = df["population"] / df["area"]
df.to_csv(sys.argv[2])
```

Make it executable with

```bash
chmod +x bin/calculate_population_density.py
```

And change the `calculate_population_density` process as follows

```groovy
process calculate_population_density {
    conda "python=3.10.6 pandas=1.5.0"

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.with_density.csv"
    script:
        """
        ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv
        """
}
```

Running the following will yield the same result as before

```bash
nextflow run main.nf --input population_area_by_country.csv
```

If we wanted to use containers instead than virtual environments for the process `calculate_population_density` to specify our software dependencies, we could have just replaced the line

```groovy
conda "python=3.10.6 pandas=1.5.0"
```
with the line

```groovy
container "docker://amancevice/pandas"
```

And then we would need to either set `docker{enabled = true}` or `singularity{enabled = true}` on our `nextflow.config` file, or we could specify the flag  `-with-docker` or `-with-singularity` from the command line.
A similar approach would work also for the process `plot_population_by_area`, using for example the container `docker:/rocker/tidyverse`.

Note that containers in the format `docker:/username/container` are pulled by Nextflow from DockerHub.
They can be used either with Docker or Singularity.



## Resource allocation

Resource allocation is very useful when our workflow is run on a high performance cluster or in the cloud.
It is possible to specify the amount of [RAM](https://www.wikiwand.com/en/Random-access_memory), the number of [CPU cores](https://www.wikiwand.com/en/Central_processing_unit), and the running time required by a process.
These values are then used by Nextflow to reserve appropriate resources, for example to submit a job to the cluster with the desired requirements.

Since we do not have a cluster environment available for this workshop, we will not try to implement an actual workflow using resource requirements (we could specify them also for a local workflow, but we would not see any effect).

An example process specifying resource requirements looks like this

```groovy
process calculate_population_density {
    // this can be in GB, MB, KB, TB, ...
    memory "2 GB"
    // this can be in second(s), minute(s), hour(s), day(s)
    // it is also possible to use abbreviations such as s, m, h, d
    time "5 days"
    // just an integer (note that this is not a string)
    cpus 4

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.with_density.csv"
    script:
        """
        ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv
        """
}
```

## The resume feature

One of the most powerful aspects of Nextflow is the resume feature.
This feature can be activated by adding the following to the `nextflow.config` file

```groovy
resume = true
```

Alternatively, it is also possible to run the Nextflow command with the `-resume` flag.

We saw before that each task is executed by Nextflow in its own directory with a long alphanumeric name.
This name is not random, but it is a 128-bit [hash number](https://www.wikiwand.com/en/Hash_function).
This number is calculated from the following values:

- Input files
- Command line string
- Container ID
- Conda environment
- Environment modules
- Any executed scripts in the bin directory

So, any time one of the following features of a process changes, the hash value and hence the working directory changes.

If a working directory with the same hash value as the one of the task to be executed is found by Nextflow, and this folder contains all the required outputs declared in the `output` block of the process, and it contains a file called `.exitcode` containing a value of zero (this is created automatically by Nextflow to save the exit code of a task execution), then the task execution is skipped and the output files present in the directory are returned as if they were produced by the current task.

This feature allows to resume the execution of a previous failed run of a workflow from the last successful task, potentially saving a lot of computing time.
A more in-depth explanation of the resume feature is available [here](https://www.nextflow.io/blog/2019/demystifying-nextflow-resume.html).

To test the resume feature by ourselves, lets modify our `main.nf` file as follows

```groovy
nextflow.enable.dsl = 2

process dumb_process {
    publishDir "../nextflow_output"

    output:
        path "out.txt"
    script:
        """
        sleep 120
        echo "Success!" > out.txt
        """
}

workflow {
    dumb_process()
}
```

Now you can run the workflow with `nextflow run main.nf`.
Our workflow just runs the process `dumb_process`, which does not take any input and declares a single output file, `out.txt`.
This execution will take 2 minutes since the script block of the process `dumb_process` contains the command `sleep 120`, which just waits for 2 minutes (120 seconds), before writing the line "Success!" to the file `out.txt`.
After the execution is complete, we can see that the line "Success!" is present in the output file by running

```bash
cat ../nextflow_output/out.txt
```

Now, to test the resume feature run again the workflow

```bash
nextflow run main.nf -resume
```

In this case, the execution will be much quicker since `process_dumb` is not actually executed, but the cached results obtained before are returned.

## nf-core pipelines

Many of the workflow that we need in our work as bioinformatics are always the same, mapping sequencing reads to a reference and then performing variant calling for example.

For such common use-cases, very efficient Nextflow pipelines are already available, so there is no need to re-invent the wheel.
Community-curated pipelines can be found on the [nf-core](https://nf-co.re) repository.
So for example for mapping reads to a reference and then performing variant calling we would use the [`nf-core/sarek`](https://nf-co.re/sarek) pipeline.
These pipelines follow a standardised format where inputs are declared by the user in a sample sheet.
You can read each pipeline's documentation in nf-core to learn more about the parameters available and the expected sample sheet.

I cannot make you run an actual nf-core pipeline now since there is not a simple and self-contained pipeline that does not require additional data like sequencing files to be run. However, once you have some files to process and you wrote an appropriate sample sheet, running the pipeline is very easy, for example:

```bash
nextflow run nf-core/sarek
```

Note that Nextflow is able to run workflows from [GitHub](https://github.com/), and `nf-core/sarek` in the command above is just a pointer to [this GitHub repository](https://github.com/nf-core/sarek).
Nextflow takes care of cloning the repository and running the `main.nf` file.

If you want to actually test running a pipeline written by someone else, run the following hello world pipeline:

```bash
nextflow run nextflow-io/hello
```

This will just print "Hello World" in different languages to the terminal.
You can explore the code for this workflow [here](https://github.com/nextflow-io/hello).

## Labels

The `label` directive is used in Nextflow to tag processes to which we want to apply specific configurations.
So for example going back to the process `calculate_population_density`

```groovy
process calculate_population_density {
    conda "python=3.10.6 pandas=1.5.0"

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.with_density.csv"
    script:
        """
        ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv
        """
}
```

Instead than hard-coding the `conda` directive into the process definition, we could have done as follows

```groovy
process calculate_population_density {
    label "pandas"

    input:
        path csv_file
    output:
        path "${csv_file.simpleName}.with_density.csv"
    script:
        """
        ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv
        """
}
```

And then in the `nextflow.config` we could have written

```groovy
withLabel:pandas {
    conda = "python=3.10.6 pandas=1.5.0"
}
```

This may seem of little benefit for a single process, but if we have many processes with similar requirements, using the `label` directive allows us to specify such requirements only once and in a central location.
So if for example we have several processes that need Python and pandas as software requirements, and we specified the requirements using the label directive, we can change the version of pandas used just by modifying what we wrote in the `nextflow.config` file, instead than having to modify the definition of many processes.

Note that the `label` directive can be used to specify many different configurations, not only software requirements.
For example, we could use it also for memory or time requirements.

## Basic challenge

It's now time to put into practice what you learned today!
Try to solve the following challenge.
The aim is to write a functioning workflow that takes in input the following sample sheet

```bash
filename,value,number_of_rows,number_of_columns
file1.txt,3,10,10
file2.txt,7,5,45
file3.txt,a_value,45,1
file4.txt,trweter,43,9
file5.txt,109,14,3
file6.txt,aaa,1,12
file7.txt,g,96,76
file8.txt,eew,11,11
file9.txt,1ww,21,34
file10.txt,45,8,2
file11.txt,jh,6,1
file12.txt,96,1,5
```

For each file in the sample sheet (under the `filename` column), the workflow should create in output a file with such name and place it in the folder `../nextflow_output/challenge`.
Each file should contain the content of the column `value` for that file.
This content should be repeated `number_of_columns` times in the same line, each instance separated by a comma.
The file should contain `number_of_rows` times the above mentioned row.
So at the end, each file should be essentially a csv file with `number_of_rows` rows and `number_of_columns` columns, without header, and with each cell containing `value`.

For example, `../nextflow_output/challenge/file1.txt` should contain

```bash
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
3,3,3,3,3,3,3,3,3,3
```

You can use as many or as few processes as you want to achieve this result. The challenge is possible using only the Nextflow features that we discussed, and a bit of bash or Python/R scripting.

You can find the solution to the challenge by disclosing the following block.

<details>
<summary>Click here to see the solution (`main.nf` file)</summary>
```groovy
nextflow.enable.dsl = 2

process create_matrix {
    conda "r-base r-tidyverse"

    publishDir "../nextflow_output/challenge"

    input:
        tuple(
            val(outname),
            val(fill_value),
            val(nrows),
            val(ncols)
        )
    output:
        path "$outname"

    script:
        """
        #!/usr/bin/env Rscript

        library("tidyverse")

        content <- rep("$fill_value", ${ncols}*${nrows})
        mat <- matrix(content, ${nrows}, ${ncols})

        df <- as.tibble(mat)

        write_csv(df, "${outname}", col_names=FALSE)
        """


}

workflow {
    Channel.fromPath( params.input )
        .splitCsv(header: true)
        .map{
            [ it["filename"], it["value"], it["number_of_rows"], it["number_of_columns"] ]
        }
        .set{ input_ch }

    create_matrix( input_ch )
}

```
</details>

The solution can be run with the f
