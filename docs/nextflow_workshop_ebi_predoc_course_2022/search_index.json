[["index.html", "Nextflow workshop - EBI predoc course 2022 Introduction to the workshop", " Nextflow workshop - EBI predoc course 2022 Saul Pierotti, PhD student in the Birney group (European Bioinformatics Institute) October 14, 2022 Introduction to the workshop This workshop aims at introducing you to the world of workflow managers, in particular the workflow manager Nextflow. This workshop was written as a practical for the EMBL-EBI predoc course, 2022 cohort. I hope that you will find it useful, and in any case I would be happy to hear your feedback on it. You can contact me at saul@ebi.ac.uk. The workshop is written with an increasing level of difficulty, so feel free to skip what is too easy for you or stop when things become too complex. Finally, I wanted to point out that in this workshop I am using the words “pipeline” and “workflow” interchangeably, so consider them as equivalent. "],["introduction.html", "1 Introduction 1.1 Prerequisites 1.2 What is Nextflow? 1.3 Learning objectives 1.4 Setup 1.5 Getting started", " 1 Introduction 1.1 Prerequisites Familiarity with the Linux shell, as well as basic programming constructs such as for/while loops and if/else statements is assumed. Familiarity with at least one scripting language such as R or Python will be beneficial. A basic knowledge of virtual environments and software containers would be helpful. Basic knowledge of git is required. 1.2 What is Nextflow? Nextflow is an open-source workflow manager consisting of a domain specific language built as a superset of the Groovy programming language, which is itself a superset of Java. The purpose of Nextflow is to make it easier to coordinate the execution of complex data analysis pipelines, and to facilitate the execution of such pipelines in cloud environments and high-performance clusters. In practice, Nextflow allows you to declare how the output of some processes in a pipeline are fed as the input of other processes, leading to the production of a final result. In addition, Nextflow allows the specification of software requirements for each process using conda environments, docker containers, and a variety of other solutions. Once a pipeline has been written in Nextflow, it can be easily scaled from a local laptop to a high-performance cluster or cloud environment without any modification, with Nextflow taking care of environment-specific commands (for example, submitting appropriate jobs to a batch scheduler). Moreover, the pipeline execution is parallelised if the dependencies among different processes allow it. A motivation for using a workflow manager such as Nextflow is also to increase the reproducibility of your data analysis. Finally, each processes in Nextflow is executed in its own unique directory, with automatic staging of the required inputs, so there is no need to think about filename collisions and file locations when developing your pipeline. Nextflow is developed and maintained by Seqera Labs. The project was started in Cedric Notredame’s lab at the Centre for Genomic Regulation (CRG) in Barcelona, Spain. Nextflow was conceived with bioinformatics as its key use-case, but it is domain-agnostic and can be used for any data-intensive workflow. 1.3 Learning objectives After following this tutorial, the learner will be autonomous in using nextflow for their own data analysis. They will understand fundamental Nextflow concepts such as processes, workflows, channels, they will be able to write a configuration file to alter the resources allocated for a processes and the software environment of execution, and they will be able to deploy a community-curated pipeline from nf-core. 1.4 Setup Install mamba, a faster alternative to conda curl -L -O &quot;https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh&quot; bash Mambaforge-$(uname)-$(uname -m).sh Setup the shell to use mamba mamba init Close and re-open your shell. Then, create a mamba environment for installing Nextflow mamba create -n nextflow nextflow Now activate the environment that you just created mamba activate nextflow Create an empty git repository in your GitHub account and clone it locally. I will pretend that this repository is called my_repo in this workshop. Don’t forget to replace this with the name of your actual repository. cd my_repo You can use a text editor of your choice to write Nextflow code. For example, you may want to use Atom. I recommend to add Nextflow language support to Atom by clicking here. Create some essential files that we will be working on inside your new repository touch main.nf touch nextflow.config 1.5 Getting started 1.5.1 General anatomy of a Nextflow workflow A Nextflow workflow is usually represented by a git repository containing all the files describing the workflow logic, configurations, and dependencies. Text files containing Nextflow code usually have the .nf extension (i.e. my_file.nf). It does not really matter how you name your files, as long as they contain valid Nextflow code, but for consistency the best practice is to call the file describing the workflow as main.nf and place it at the root of your repository. The main.nf file can access code from other Nextflow scripts, called sub-workflows, that can have arbitrary names. The best practice is to place such additional Nextflow scripts under the folder workflows in the root of the repository. So for example an additional Nextflow script could be named workflows/preprocessing.nf. We will explore later the usage of sub-workflows, so don’t worry if you don’t understand their purpose yet. The configurations for the pipeline need to be placed in a file called nextflow.config at the root of the repository. Note that differently from what was said before regarding main.nf, this name is mandatory for Nextflow to properly recognise the file. The nextflow.config file can contain for example resource requirements, execution, parameters, and metadata about your workflow. We will explore more in depth the usage of configurations files in a later section. Scripts (in any language, for example R or Python) that are used in the execution of specific processes in your pipeline should be placed in the folder bin. Scripts placed here are automatically recognise by Nextflow during task execution without the need to specify their full path. Additional files needed for your workflow, for example a table containing some metadata for your samples, are usually placed under the folder assets. However this is not required and many workflows do not have an assets folder. Inputs to the workflow are usually defined in a so-called “sample sheet” that should contain the (preferably) absolute file path to the input files end eventual sample-specific parameters for task execution. Best practice is to have your sample sheet formatted as a csv file with an appropriate header. Note that the sample sheet is not part of the pipeline itself. The absolute path to the sample sheet is usually provided to Nextflow either as a command-line parameter or in the nextflow.config file. In order to execute your workflow, you would run (do not do it now, we still need to write the workflow) nextflow run main.nf at the root of your repository if your workflow is in a file named main.nf. This would prompt Nextflow to read the main.nf file and eventual sub-workflows, extract the workflow logic from it, load the configurations in nextflow.config, and coordinate the execution of the different steps in your pipeline to produce the final result. 1.5.2 Nextflow domain specific language versions Before starting, it is important to know that Nextflow as it was initially developed is referred to as Domain Specific Language 1 (DSL1). A major change in the Nextflow syntax was done by its developers, and the new syntax is referred to as DSL 2. In this workshop we will be using exclusively the new DSL2 syntax. For this reason, you need to add the following line at the top of your main.nf file: nextflow.enable.dsl = 2 So if in the future you will find yourself looking at a Nextflow workflow which is written very differently from what you are used to, it will probably be written according to DSL1 instead of DSL2. 1.5.3 Core concepts A Nextflow workflow is defined by a set of processes, which execute a single task each, which are coordinate by one or more workflows. So for example there may be a processes called align_fastq that takes in input a fastq file and a reference genome and uses the software bwa-mem2 to align it, producing an aligned cram file in output. A processes defines as a minimum the command to be executed. Outputs are typically defined but may be omitted if the process is run for its side effects. Inputs are usually also defined but there may be processes that do not need any input and instead perform a static computation. Many additional parameters can be specified for a process, for example the software needed, or how much memory or time is required for its execution. We will explore processes and their definition more in detail in a later section. If processes determine what is to be executed and how, workflows instead determine the logic of execution and how different processes communicate with each other. So for example there may be a workflow called SNP_CALLING that takes a fastq file in input, uses the process align_fastq to obtain an aligned cram file, then gives that cram file in input to another process called gatk_call that uses it to create a vcf file containing genetic variants. Processes communicate with each other using so-called “channels”, which are unidirectional First-In-First-Out (FIFO) queues. This means that a channel is populated with a set of elements (for example, files) produced by a process in the order in which they are produced. Then, these elements are consumed one by one by another process in the same order. Several ways to manipulate, merge, and split channel exist, and we will explore them later. So for example the workflow that I described above may define an input channel containing the fastq files in input, and a channel connecting the output of align_fastq to the input of gatk_call. Note that channels are unidirectional: communication happens only in one direction, from outputs to inputs. 1.5.4 Your first workflow Now that you know the basics of what a Nextflow workflow is, we will write a simple workflow and we will execute it. First of all, open the file main.nf that you created before at the root of your repository in a text editor and add the following to it // this is a comment process say_hello { // comments can be written anywhere output: path &quot;hello_world.txt&quot; script: &quot;&quot;&quot; echo &quot;This is the EBI predoc course&quot; &gt; hello_world.txt &quot;&quot;&quot; } workflow { say_hello() say_hello.out.view() } If you want to see how your main.nf should look like at this stage open this hidden section Code nextflow.enable.dsl = 2 // this is a comment process say_hello { // comments can be written anywhere output: path &quot;hello_world.txt&quot; script: &quot;&quot;&quot; echo &quot;This is the EBI predoc course&quot; &gt; hello_world.txt &quot;&quot;&quot; } workflow { say_hello() say_hello.out.view() } Now open the file nextflow.config and add the following line // you can also put comments in nextflow.config workDir = &quot;../nextflow_workdir&quot; Now run the workflow executing from the root of your repository the following nextflow run main.nf You should see a bunch of information about your workflow and finally you will see the name of the file we just created, hello_world.txt, appear on your terminal. Note that the full path to the file will be printed in your terminal, which will look something like /home/myname/nextflow_workdir/00/fjdanurew9gihwepw1455idusodhfweioru/hello_world.txt. Let’s now examine the code step by step: In nextflow.config we declared our working directory to be ../nextflow_workdir with the keyword workDir. The working directory is where Nextflow actually stores your files during intermediate processing. Lines starting with // define a comment in groovy, and they are ignored. The keyword process defines a process, with the statement that follows defining the process name. So process say_hello creates a process named say_hello. Curly braces enclose a block of code, so process say_hello {&lt;some code&gt;} is understood by Nextflow as defining &lt;some code&gt; to belong to the process named say_hello. The keyword output: when used inside of a process defines the expected outputs that that process should produce. If the declared output is absent at the end of the process’ execution the execution fails. Several types of outputs can be defined, and path is one of them. The keyword path defines what comes after it to be a file. Output files are named in the output block after the path qualifier. They should be placed inside a string. Strings are enclosed in quotes in groovy (\"this is a string\"). The script: keyword defines the actual command to be executed in the process. The command should be provided as a string. Since commands can be long, here a multi-line string is used. In groovy, multi-line strings are enclosed in three quotes &quot;&quot;&quot; this is a multi-line string it is very long &quot;&quot;&quot; What is declared in the script: block is executed in the shell by default, so we should write bash code there The command that we wrote, echo \"This is the EBI predoc course\" &gt; hello_world.txt, creates a file called hello_world.txt containing the string \"This is the EBI predoc course\" The keyword workflow defines a workflow, that we left unnamed in this case. The last workflow to be written in your main.nf file is automatically executed by Nextflow. The workflow that we defined executes the process say_hello The output of the process say_hello is captured by say_hello.out, which is a channel. In this case it will contain just the file hello_world.txt The operator view() just echoes to the terminal the content of the channel. In this case it prints hello_world.txt to our shell. So to put everything together, when you ran nextflow run main.nf Nextflow read the main.nf file, it found the last workflow, which we left unnamed, and executed it. Some additional notes: It is possible to write path(\"a_file.txt\") or path \"a_file.txt\", they are equivalent statements There is no need to think about where we are creating our files, Nextflow under the hood creates a private directory for each execution and takes care of moving files around as needed by other processes It is common practice to chain different operators on a channel It is possible to chain operators on separate lines The following is equivalent to say_hello.out.view() say_hello.out .view() Note that spaces and tab charachters are just for visual clarity and are not required Explore the working directory ../nextflow_workdir. It contains a directory with a strange alphanumeric name like 00/fjdanurew9gihwepw1455idusodhfweioru (NOTE: yours will have a different name). One different directory like this is created by Nextflow for the execution of each task (a task is an instance of a process, a process is the definition itself, while a task is created each time that process is executed on different inputs). Inside the directory with the strange name, you will see the file that we just created, hello_world.txt Read the content of the file with cat hello_world.txt. You should see This is the EBI predoc course in your terminal. "],["basic-features.html", "2 Basic features 2.1 Multiple processes and publication of workflow outputs 2.2 Creating channels and specifying pipeline parameters 2.3 Multiple input files 2.4 Using a sample sheet and value channels 2.5 Software dependencies 2.6 Resource allocation 2.7 The resume feature 2.8 nf-core pipelines 2.9 Labels 2.10 Basic challenge", " 2 Basic features 2.1 Multiple processes and publication of workflow outputs Let’s now step up a bit the complexity and write a workflow containing two processes that communicate with each other. Add the following process to your main.nf file. process duplicate_lines { publishDir &quot;../nextflow_output/duplicate_lines&quot; input: path my_file output: path &quot;${my_file.simpleName}.duplicated.txt&quot; script: &quot;&quot;&quot; cat $my_file $my_file &gt; ${my_file.simpleName}.duplicated.txt &quot;&quot;&quot; } And modify your workflow block as follows workflow { say_hello() duplicate_lines( say_hello.out ) } If you want to see how your main.nf should look like at this stage open this hidden section Code nextflow.enable.dsl = 2 // this is a comment process say_hello { // comments can be written anywhere output: path &quot;hello_world.txt&quot; script: &quot;&quot;&quot; echo &quot;This is the EBI predoc course&quot; &gt; hello_world.txt &quot;&quot;&quot; } process duplicate_lines { publishDir &quot;../nextflow_output/duplicate_lines&quot; input: path my_file output: path &quot;${my_file.simpleName}.duplicated.txt&quot; script: &quot;&quot;&quot; cat $my_file $my_file &gt; ${my_file.simpleName}.duplicated.txt &quot;&quot;&quot; } workflow { say_hello() duplicate_lines( say_hello.out ) } Now run again the workflow with nextflow run main.nf. This time your workflow will not print anything since we omitted the view() operator. However, if you type in your terminal the following cat ../nextflow_output/duplicate_lines/hello_world.duplicated.txt You should see the following This is the EBI predoc course This is the EBI predoc course So as you see, the content of the hello_world.txt file that we created before has been duplicated so that it appears twice and it has been placed on a new file, ../nextflow_output/duplicate_lines/hello_world.duplicated.txt. Let’s analyse what happened this time: We created another process called duplicate_lines which duplicates a given file thanks to its script command cat $my_file $my_file &gt; ${my_file.simpleName}.duplicated.txt duplicate_lines declares an input: block. The input block is similar to the output block that we saw before in that it can use the path qualifier to declare the input to be a file. However, what comes after path does not need to be a real filename, it is just a variable name (note that it is not quoted). Nextflow replaces that variable with the real name of the file given in input. In the script and output blocks we can refer to the inputs by specifying $my_file. Note that we need to use the same name used in the input declaration. It is possible to enclose the variable name in curly braces to demark it from other text. So ${my_file} is equivalent to $my_file. We applied the operator simpleName to the variable $my_file by writing ${my_file.simpleName}. This removes the path and the extension from my_file, so that we can use it to name our output file (if $my_file contains the value \"/some/path/hello_world.txt\", then ${my_file.simpleName} contains only hello_world). We used a new directive in the process duplicate_lines, called publishDir. This specifies the folder where the output of the process should be placed at the end of the execution. This is usually done to put the final outputs of your workflow in a meaningful directory structure. In our case, publishDir \"../nextflow_output/duplicate_lines\" places ${my_file.simpleName}.duplicated.txt in the folder ../nextflow_output/duplicate_lines In the workflow block we called the process duplicate_lines with say_hello.out as an argument. So the output of say_hello is used as an input for duplicate_lines. Note that the number of arguments provided to a process must match the number of arguments declared in its input block. 2.2 Creating channels and specifying pipeline parameters The workflow that we wrote works as expected but it is not very useful since we cannot specify dynamically its input files. We will now learn how to use pipeline parameters to specify an external input file for the whole workflow and how to use channel factories to feed it to our processes. First create a text file called ../nextflow_inputs/a_file.txt containing the string \"This is the file content\" mkdir ../nextflow_inputs echo &quot;This is the file content&quot; &gt; ../nextflow_inputs/a_file.txt Modify the workflow as follows workflow { Channel.fromPath( params.input_file ).set{ input_ch } duplicate_lines( input_ch ) } If you want to see how your main.nf should look like at this stage open this hidden section Code nextflow.enable.dsl = 2 // this is a comment process say_hello { // comments can be written anywhere output: path &quot;hello_world.txt&quot; script: &quot;&quot;&quot; echo &quot;This is the EBI predoc course&quot; &gt; hello_world.txt &quot;&quot;&quot; } process duplicate_lines { publishDir &quot;../nextflow_output/duplicate_lines&quot; input: path my_file output: path &quot;${my_file.simpleName}.duplicated.txt&quot; script: &quot;&quot;&quot; cat $my_file $my_file &gt; ${my_file.simpleName}.duplicated.txt &quot;&quot;&quot; } workflow { Channel.fromPath( params.input_file ).set{ input_ch } duplicate_lines( input_ch ) } Now open the file nextflow.config and add the following params { input_file = &quot;../nextflow_inputs/a_file.txt&quot; } If you want to see how your nextflow.config should look like at this stage open this hidden section Code // you can also put comments in nextflow.config workDir = &quot;../nextflow_workdir&quot; params { input_file = &quot;../nextflow_inputs/a_file.txt&quot; } Now run again the workflow with nextflow run main.nf. This time if you examine the folder ../nextflow_output/duplicate_lines you will find a file called a_file.duplicated.txt. Let’s see it’s content. cat ../nextflow_output/duplicate_lines/a_file.duplicated.txt This should print This is the file content This is the file content So the content of the file that we created before, a_file.txt was duplicated and placed in the file a_file.duplicated.txt in the folder ../nextflow_output/duplicate_lines. Let’s examine what happened: In nextflow.config, we declared a params block. Any variable written inside this block, like input_file = \"../nextflow_inputs/a_file.txt\", is accessible in the workflow by writing params.&lt;variable_name&gt;, so params.input_file in this case. In main.nf, we wrote Channel.fromPath( params.input_file ). This uses the channel factory Channel.fromPath to create a new channel using the content of params.input_file. The Channel.fromPath factory interprets its argument to be the path to a file. After Channel.fromPath( params.input_file ) we added set{ input_ch } (note the curly braces: it is a closure, we will explore later what does it mean). This assigns to the newly created channel the name input_ch We then used the channel input_ch as an input for the process duplicate_lines in the statement duplicate_lines( input_ch ). Since the channel input_ch contains the variable params.input_file, which we declared to contain the value ../nextflow_inputs/a_file.txt, this file is given in input to duplicate_lines duplicate_lines performed its function as before, putting its output in ../nextflow_output/duplicate_lines Instead than hard-coding parameters in nextflow.config, it is also possible to pass them on the command line. So for example we could have omitted the params block in nextflow.config and run nextflow run main.nf --input_file ../nextflow_inputs/a_file.txt This produces the same result. Note that pipeline parameters need to be specified with two prepending dashes (--my_parameter). This differentiates them from command line options for Nextflow itself, such as -dry-run, which use a single dash. 2.3 Multiple input files We are now able to process a single file with our pipeline, but what if we have many files that we need to process in the same way? One approach for this is to use a glob pattern as an input. Let’s create a bunch of files to use in input echo &quot;This is content of file 1&quot; &gt; ../nextflow_inputs/set_of_files_1.txt echo &quot;This is content of file 2&quot; &gt; ../nextflow_inputs/set_of_files_2.txt echo &quot;This is content of file 3&quot; &gt; ../nextflow_inputs/set_of_files_3.txt echo &quot;This is content of file 4&quot; &gt; ../nextflow_inputs/set_of_files_4.txt echo &quot;This is content of file 5&quot; &gt; ../nextflow_inputs/set_of_files_5.txt echo &quot;This is content of file 6&quot; &gt; ../nextflow_inputs/set_of_files_6.txt echo &quot;This is content of file 7&quot; &gt; ../nextflow_inputs/set_of_files_7.txt echo &quot;This is content of file 8&quot; &gt; ../nextflow_inputs/set_of_files_8.txt Now we just need to modify input_file in nextflow.config params { input_file = &quot;../nextflow_inputs/set_of_files_*.txt&quot; } If you want to see how your nextflow.config should look like at this stage open this hidden section Code // you can also put comments in nextflow.config workDir = &quot;../nextflow_workdir&quot; params { input_file = &quot;../nextflow_inputs/set_of_files_*.txt&quot; } Now run again the workflow with nextflow run main.nf. This time if you examine the output folder with ls ../nextflow_output/duplicate_lines you will find a set of files a_file.duplicated.txt set_of_files_1.duplicated.txt set_of_files_2.duplicated.txt set_of_files_3.duplicated.txt set_of_files_4.duplicated.txt set_of_files_5.duplicated.txt set_of_files_6.duplicated.txt set_of_files_7.duplicated.txt set_of_files_8.duplicated.txt Like before, each of them will contain the duplicated version of the original files. Let’s examine what happened: We changed params.input_ch to contain a glob pattern. This is expanded by Nextflow to yield a list of matching files. The matching files are fed one by one via the channel input_ch to the process duplicate_lines duplicate_lines operates independently but in parallel on all the inputs, producing the output files. Each task is executed in its own private directory. 2.4 Using a sample sheet and value channels Using glob patterns for specifying samples is useful and quick, but what if we want to specify input files that live in many different directories with very different names? And if we want some files to be processed in pairs with other specific files, or with specific parameters? For such use cases a sample sheet is the easiest solution and it is the recommended way to specify workflow inputs. A sample sheet is just a csv file with one row per file to be processed, and with each column specifying either a file or a parameter. Create a sample sheet called ../nextflow_inputs/samplesheet.csv by running the following command. echo &quot;sample,content&quot; &gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_1.txt,csv_content_1&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_2.txt,csv_content_2&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_3.txt,csv_content_3&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_4.txt,csv_content_4&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_5.txt,csv_content_5&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_6.txt,csv_content_6&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_7.txt,csv_content_7&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv echo &quot;$(pwd)/../nextflow_inputs/set_of_files_8.txt,csv_content_8&quot; &gt;&gt; ../nextflow_inputs/samplesheet.csv Now create a process called append_to_file process append_to_file { publishDir &quot;../nextflow_output/append_to_file&quot; input: path my_file val my_val output: path &quot;${my_file.simpleName}.appended.txt&quot; script: &quot;&quot;&quot; cat $my_file &gt; ${my_file.simpleName}.appended.txt echo &quot;$my_val&quot; &gt;&gt; ${my_file.simpleName}.appended.txt &quot;&quot;&quot; } And modify the workflow as follows workflow { Channel.fromPath( params.samplesheet ) .splitCsv( header: true ) .set{ input_ch } input_ch.map{ it[&quot;sample&quot;] }.set{ sample_ch } input_ch.map{ it[&quot;content&quot;] }.set{ content_ch } duplicate_lines( sample_ch ) append_to_file( duplicate_lines.out, content_ch ) } If you want to see how your main.nf file should look like at this stage open this hidden section Code nextflow.enable.dsl = 2 // this is a comment process say_hello { // comments can be written anywhere output: path &quot;hello_world.txt&quot; script: &quot;&quot;&quot; echo &quot;This is the EBI predoc course&quot; &gt; hello_world.txt &quot;&quot;&quot; } process duplicate_lines { publishDir &quot;../nextflow_output/duplicate_lines&quot; input: path my_file output: path &quot;${my_file.simpleName}.appended.txt&quot; script: &quot;&quot;&quot; cat $my_file $my_file &gt; ${my_file.simpleName}.duplicated.txt &quot;&quot;&quot; } process append_to_file { publishDir &quot;../nextflow_output/append_to_file&quot; input: path my_file val my_val output: path &quot;${my_file.simpleName}.duplicated.txt&quot; script: &quot;&quot;&quot; cat $my_file &gt; ${my_file.simpleName}.appended.txt echo &quot;$my_val&quot; &gt;&gt; ${my_file.simpleName}.appended.txt &quot;&quot;&quot; } workflow { Channel.fromPath( params.samplesheet ) .splitCsv( header: true ) .set{ input_ch } input_ch.map{ it[&quot;sample&quot;] }.set{ sample_ch } input_ch.map{ it[&quot;content&quot;] }.set{ content_ch } duplicate_lines( sample_ch ) append_to_file( duplicate_lines.out, content_ch ) } Now we need to modify the params block in your nextflow.config params { samplesheet = &quot;../nextflow_inputs/samplesheet.csv&quot; } If you want to see how your nextflow.config should look like at this stage open this hidden section Code // you can also put comments in nextflow.config workDir = &quot;../nextflow_workdir&quot; params { samplesheet = &quot;../nextflow_inputs/samplesheet.csv&quot; } Now run again the workflow with nextflow run main.nf. This time if you examine the output folder with ls ../nextflow_output/append_to_file you will find a set of files set_of_files_1.append_to_file.txt set_of_files_2.append_to_file.txt set_of_files_3.append_to_file.txt set_of_files_4.append_to_file.txt set_of_files_5.append_to_file.txt set_of_files_6.append_to_file.txt set_of_files_7.append_to_file.txt set_of_files_8.append_to_file.txt Taking as an example the first one, set_of_files_1.append_to_file.txt we would expect it to contain This is content of file 1 This is content of file 1 csv_content_1 However, most probably it will contain something like This is content of file 1 This is content of file 1 csv_content_3 So the first two lines have the correct number but the last line shows the wrong element! What’s more, the element shown on the last line can be different on each run (try!). We said before that channels are guaranteed to emit elements in the same order that they are received, so we should have the correct matches between files and values. What happened? What happened is that while channels are indeed guaranteed to preserve order, processes and some operators are not because of their asynchronous and parallel nature. In our case, the process duplicate_lines receives the files in the correct order, but it is not guaranteed to emit them in the same order. This is because all the file are processed in parallel, and for example set_of_files_5.append_to_file.txt could be processed and emitted some milliseconds earlier that set_of_files_1.append_to_file.txt, and so the order would be altered. A better approach when we want multiple channels to be used by one process with some predictable matching is to join channels using matching keys. We will explore it in a later section. For now, let’s examine what happened in this version of our workflow: We created the process append_to_file, which takes in input a file (path my_file) and a value (val my_val). This process copies the content of my_file to ${my_file.simpleName}.append_to_file.txt and then appends to the newly created file the content of my_val We defined the parameter samplesheet in nextflow.config We modified the workflow so that we use params.samplesheet to create a channel with Channel.fromPath We applied the operator splitCsv to this channel containing the samplesheet, using the option header: true. This operator reads the file contained in the channel assuming it to be a csv file with an header line. Then it produces in output another channel containing, for each row in the samplesheet, a groovy map (what in Python would be called a dictionary). This map contains a key for each element in the csv header, and a value corresponding to the value of that column in the current line of the csv file. We apply the map operator (.map{ it[\"sample\"] }) to the resulting channel The map operator is follwed by a closure ({ it[\"sample\"] }) You can think of a closure as an unnamed function (like a lambda function in Python) The map operator calls this unnamed function for each element in the channel, and creates another channel containing the respective function outputs Closure understand the implicit variable it, which represents whatever imput is given to the function If an explicit return statement is not present in the closure, the last evaluated expression is returned by the closure ( it[\"sample\"] in this case) Since splitCsv created a dictionary for each item in the samplesheet, map{ it[\"sample\"] } replaces that dictionary (it in the closure) with the content of the value contained in it under the key \"sample\" input_ch is used again in input_ch.map{ it[\"content\"] }.set{ content_ch } to extract the value of content for each sample from the samplesheet Channels can be used many times, and each time all the files contained in it are processed. So both input_ch.map{ it[\"content\"] }.set{ content_ch } and input_ch.map{ it[\"sample\"] }.set{ sample_ch } process every element in input_ch We created the channels sample_ch and content_ch using the set operator. They contain respectively the sample files and the content values related to them. The order of processing in a channel is guaranteed, so we can be sure that each sample will be paired with the correct content value We fed sample_ch to duplicate_lines We fed the output of duplicate_lines to append_to_file, together with the values in content_ch append_to_file appended the value of content_ch to the output of duplicate_lines, creating the final result Note that we introduced a new qualifier: val. The val qualifier specifies a value, differently from the path qualifier that specifies a file. If I write val variable_name, then variable_name can contain something like a string or a number, depending on what is fed to the process input. It is also possible to manually create value channels using the channel factory Channel.value. Take as an example the following workflow{ Channel.value(1, 2, 3).view() } This would produce a channel containing in order the values 1, 2, and 3. These would then be printed one by one to the terminal by the view operator. 2.5 Software dependencies We can do many things with just shell scripts, but for real-word scenarios we would probably need to use some additional library or tool. Software dependencies for a process can be specified in Nextflow in two main ways: using software containers or virtual environments. A software container is like a shipping container: it is a monolithic block of code and data that contains everything needed to run a specific application, including the operating system (but not the kernel, the host kernel is used). The most popular container engine (the software that is used to actually run a container) is Docker. In high-performance clusters, for security reasons Docker is often discouraged or forbidden. A popular alternative container engine used in bioinformatics is Singularity. The good news is that Singularity is also able to run containers created with Docker, so we can consider them equivalent for our purpose. For popular tools, a container has most probably already been created by someone and put in a public container registry like DockerHub. In this case, we can provide to Nextflow just a link to the container and it will do the rest. If instead we need to use a custom container, we would need to write an appropriate description file for it (called a “Dockerfile”), use it to actually build the container, and then upload the container to DockerHub. This last use-case will not be treated in this workshop, and we will limit ourselves to the use of pre-built containers. On the other hand, virtual environments are a more lightweight and modifiable alternative to containers. They rely on the host operating system but they allow to define additional software. Nextflow is compatible with Conda environments, and can also use the faster Mamba implementation of Conda. It is worth noting that while Conda/Mamba are typically thought of as package managers for Python, they are actually able to install also R packages, the R interpreter itself, stand-alone tools, and even GPU drivers. Differently from containers, environments do not need to be pre-built to be used by Nextflow. It is possible to just define the software that we need and Nextflow will take care to build an appropriate environment with it. Let’s now write a workflow that makes use of virtual environments. First download a csv file containing a set of countries, their population, and their area by running the following wget https://raw.githubusercontent.com/saulpierotti-ebi/saulpierotti-ebi.github.io/main/assets/population_area_by_country.csv Now modify your main.nf file and make it look like this nextflow.enable.dsl = 2 process calculate_population_density { conda &quot;python=3.10.6 pandas=1.5.0&quot; input: path csv_file output: path &quot;${csv_file.simpleName}.with_density.csv&quot; script: &quot;&quot;&quot; #!/usr/bin/env Python import pandas df = pandas.read_csv(&quot;${csv_file}&quot;) df[&quot;density&quot;] = df[&quot;population&quot;] / df[&quot;area&quot;] df.to_csv(&quot;${csv_file.simpleName}.with_density.csv&quot;) &quot;&quot;&quot; } process plot_population_by_area { publishDir &quot;../nextflow_output/plots&quot; conda &quot;r-base=4.1.3 r-tidyverse=1.3.2&quot; input: path csv_file output: path &quot;${csv_file.simpleName}.pdf&quot; script: &quot;&quot;&quot; #!/usr/bin/env Rscript library(&quot;tidyverse&quot;) df &lt;- read_csv(&quot;${csv_file}&quot;) ggplot(df, aes(x=area, y=population, label=country, size=density)) + geom_text() ggsave(&quot;${csv_file.simpleName}.pdf&quot;) &quot;&quot;&quot; } workflow { input_ch = Channel.fromPath( params.input ) calculate_population_density( input_ch ) plot_population_by_area( calculate_population_density.out ) } Modify your nextflow.config by adding the following conda { // use the faster mamba solver instead of conda useMamba = true } If you want to see how your nextflow.config should look like at this stage open this hidden section Code // you can also put comments in nextflow.config workDir = &quot;../nextflow_workdir&quot; params { samplesheet = &quot;../nextflow_inputs/samplesheet.csv&quot; } conda { // use the faster mamba solver instead of conda useMamba = true } Now run the workflow as follows nextflow run main.nf --input population_area_by_country.csv This time the workflow will take significantly longer to run since Nextflow needs to create two mamba environments. However, this will affect only the first run of the workflow, since once an environment is created it can be used multiple times. Now open the file ../nextflow_output/plots/population_area_by_country.pdf. It will contain a plot of the population of different countries versus their area, with the size proportional to their population density. Let’s explore what happened: We specified to Nextflow to use mamba instead than conda when using the conda directive (with conda{use.mamba = true} in nextflow.config) As before, we used a command-line flag to pass an input file to the workflow (--input population_area_by_country.csv) We used population_area_by_country.csv as an input for the process calculate_population_density, which uses Python code to calculate the population density of a country from its area and total population The output of calculate_population_density is fed to the process plot_population_by_area, which uses R code to produce the plot The conda directive used in these processes defines the software dependencies needed, in this case Python and pandas for the first process and R and tidyverse for the second process. Software versions can be specified with an equal sign after the software name. Packages are sourced from the repository Anaconda. Another thing to note here: the script directive normally interprets the code written in it as bash code, but if an sh-bang (!#) is included in the first line of the script, then the software specified in the sh-bang is used to run the script section (so !#/usr/bin/env Python tells to Nextflow to interpret the script section as Python code). A different approach to reach the same goal is to write a separate Python or R script, place it in the bin folder, and then execute it with a bash command in the script directive. So for example you can create a file called bin/calculate_population_density.py with the following content import pandas import sys df = pandas.read_csv(sys.argv[1]) df[&quot;density&quot;] = df[&quot;population&quot;] / df[&quot;area&quot;] df.to_csv(sys.argv[2]) Make it executable with chmod +x bin/calculate_population_density.py And change the calculate_population_density process as follows process calculate_population_density { conda &quot;python=3.10.6 pandas=1.5.0&quot; input: path csv_file output: path &quot;${csv_file.simpleName}.with_density.csv&quot; script: &quot;&quot;&quot; ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv &quot;&quot;&quot; } Running the following will yield the same result as before nextflow run main.nf --input population_area_by_country.csv If we wanted to use containers instead than virtual environments for the process calculate_population_density to specify our software dependencies, we could have just replaced the line conda &quot;python=3.10.6 pandas=1.5.0&quot; with the line container &quot;docker://amancevice/pandas&quot; And then we would need to either set docker{enabled = true} or singularity{enabled = true} on our nextflow.config file, or we could specify the flag -with-docker or -with-singularity from the command line. A similar approach would work also for the process plot_population_by_area, using for example the container docker:/rocker/tidyverse. Note that containers in the format docker:/username/container are pulled by Nextflow from DockerHub. They can be used either with Docker or Singularity. 2.6 Resource allocation Resource allocation is very useful when our workflow is run on a high performance cluster or in the cloud. It is possible to specify the amount of RAM, the number of CPU cores, and the running time required by a process. These values are then used by Nextflow to reserve appropriate resources, for example to submit a job to the cluster with the desired requirements. Since we do not have a cluster environment available for this workshop, we will not try to implement an actual workflow using resource requirements (we could specify them also for a local workflow, but we would not see any effect). An example process specifying resource requirements looks like this process calculate_population_density { // this can be in GB, MB, KB, TB, ... memory &quot;2 GB&quot; // this can be in second(s), minute(s), hour(s), day(s) // it is also possible to use abbreviations such as s, m, h, d time &quot;5 days&quot; // just an integer (note that this is not a string) cpus 4 input: path csv_file output: path &quot;${csv_file.simpleName}.with_density.csv&quot; script: &quot;&quot;&quot; ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv &quot;&quot;&quot; } 2.7 The resume feature One of the most powerful aspects of Nextflow is the resume feature. This feature can be activated by adding the following to the nextflow.config file resume = true Alternatively, it is also possible to run the Nextflow command with the -resume flag. We saw before that each task is executed by Nextflow in its own directory with a long alphanumeric name. This name is not random, but it is a 128-bit hash number. This number is calculated from the following values: Input files Command line string Container ID Conda environment Environment modules Any executed scripts in the bin directory So, any time one of the following features of a process changes, the hash value and hence the working directory changes. If a working directory with the same hash value as the one of the task to be executed is found by Nextflow, and this folder contains all the required outputs declared in the output block of the process, and it contains a file called .exitcode containing a value of zero (this is created automatically by Nextflow to save the exit code of a task execution), then the task execution is skipped and the output files present in the directory are returned as if they were produced by the current task. This feature allows to resume the execution of a previous failed run of a workflow from the last successful task, potentially saving a lot of computing time. A more in-depth explanation of the resume feature is available here. To test the resume feature by ourselves, lets modify our main.nf file as follows nextflow.enable.dsl = 2 process dumb_process { publishDir &quot;../nextflow_output&quot; output: path &quot;out.txt&quot; script: &quot;&quot;&quot; sleep 120 echo &quot;Success!&quot; &gt; out.txt &quot;&quot;&quot; } workflow { dumb_process() } Now you can run the workflow with nextflow run main.nf. Our workflow just runs the process dumb_process, which does not take any input and declares a single output file, out.txt. This execution will take 2 minutes since the script block of the process dumb_process contains the command sleep 120, which just waits for 2 minutes (120 seconds), before writing the line “Success!” to the file out.txt. After the execution is complete, we can see that the line “Success!” is present in the output file by running cat ../nextflow_output/out.txt Now, to test the resume feature run again the workflow nextflow run main.nf -resume In this case, the execution will be much quicker since process_dumb is not actually executed, but the cached results obtained before are returned. 2.8 nf-core pipelines Many of the workflow that we need in our work as bioinformatics are always the same, mapping sequencing reads to a reference and then performing variant calling for example. For such common use-cases, very efficient Nextflow pipelines are already available, so there is no need to re-invent the wheel. Community-curated pipelines can be found on the nf-core repository. So for example for mapping reads to a reference and then performing variant calling we would use the nf-core/sarek pipeline. These pipelines follow a standardised format where inputs are declared by the user in a sample sheet. You can read each pipeline’s documentation in nf-core to learn more about the parameters available and the expected sample sheet. I cannot make you run an actual nf-core pipeline now since there is not a simple and self-contained pipeline that does not require additional data like sequencing files to be run. However, once you have some files to process and you wrote an appropriate sample sheet, running the pipeline is very easy, for example: nextflow run nf-core/sarek Note that Nextflow is able to run workflows from GitHub, and nf-core/sarek in the command above is just a pointer to this GitHub repository. Nextflow takes care of cloning the repository and running the main.nf file. If you want to actually test running a pipeline written by someone else, run the following hello world pipeline: nextflow run nextflow-io/hello This will just print “Hello World” in different languages to the terminal. You can explore the code for this workflow here. 2.9 Labels The label directive is used in Nextflow to tag processes to which we want to apply specific configurations. So for example going back to the process calculate_population_density process calculate_population_density { conda &quot;python=3.10.6 pandas=1.5.0&quot; input: path csv_file output: path &quot;${csv_file.simpleName}.with_density.csv&quot; script: &quot;&quot;&quot; ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv &quot;&quot;&quot; } Instead than hard-coding the conda directive into the process definition, we could have done as follows process calculate_population_density { label &quot;pandas&quot; input: path csv_file output: path &quot;${csv_file.simpleName}.with_density.csv&quot; script: &quot;&quot;&quot; ./calculate_population_density.py $csv_file ${csv_file.simpleName}.with_density.csv &quot;&quot;&quot; } And then in the nextflow.config we could have written withLabel:pandas { conda = &quot;python=3.10.6 pandas=1.5.0&quot; } This may seem of little benefit for a single process, but if we have many processes with similar requirements, using the label directive allows us to specify such requirements only once and in a central location. So if for example we have several processes that need Python and pandas as software requirements, and we specified the requirements using the label directive, we can change the version of pandas used just by modifying what we wrote in the nextflow.config file, instead than having to modify the definition of many processes. Note that the label directive can be used to specify many different configurations, not only software requirements. For example, we could use it also for memory or time requirements. 2.10 Basic challenge It’s now time to put into practice what you learned today! Try to solve the following challenge. The aim is to write a functioning workflow that takes in input the following sample sheet filename,value,number_of_rows,number_of_columns file1.txt,3,10,10 file2.txt,7,5,45 file3.txt,a_value,45,1 file4.txt,trweter,43,9 file5.txt,109,14,3 file6.txt,aaa,1,12 file7.txt,g,96,76 file8.txt,eew,11,11 file9.txt,1ww,21,34 file10.txt,45,8,2 file11.txt,jh,6,1 file12.txt,96,1,5 For each file in the sample sheet (under the filename column), the workflow should create in output a file with such name and place it in the folder ../nextflow_output/challenge. Each file should contain the content of the column value for that file. This content should be repeated number_of_columns times in the same line, each instance separated by a comma. The file should contain number_of_rows times the above mentioned row. So at the end, each file should be essentially a csv file with number_of_rows rows and number_of_columns columns, without header, and with each cell containing value. For example, ../nextflow_output/challenge/file1.txt should contain 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 3,3,3,3,3,3,3,3,3,3 You can use as many or as few processes as you want to achieve this result. The challenge is possible using only the Nextflow features that we discussed, and a bit of bash or Python/R scripting. You can find the solution to the challenge by disclosing the following block. Click here to see the solution (main.nf file) nextflow.enable.dsl = 2 process create_matrix { conda &quot;r-base r-tidyverse&quot; publishDir &quot;../nextflow_output/challenge&quot; input: tuple( val(outname), val(fill_value), val(nrows), val(ncols) ) output: path &quot;$outname&quot; script: &quot;&quot;&quot; #!/usr/bin/env Rscript library(&quot;tidyverse&quot;) content &lt;- rep(&quot;$fill_value&quot;, ${ncols}*${nrows}) mat &lt;- matrix(content, ${nrows}, ${ncols}) df &lt;- as.tibble(mat) write_csv(df, &quot;${outname}&quot;, col_names=FALSE) &quot;&quot;&quot; } workflow { Channel.fromPath( params.input ) .splitCsv(header: true) .map{ [ it[&quot;filename&quot;], it[&quot;value&quot;], it[&quot;number_of_rows&quot;], it[&quot;number_of_columns&quot;] ] } .set{ input_ch } create_matrix( input_ch ) } If the sample sheet is saved as ../nextflow_output/samplesheet.csv, the solution can be run with the following command nextflow run main.nf --input ../nextflow_output/samplesheet.csv "],["a-more-advanced-example.html", "3 A more advanced example 3.1 Some more operators 3.2 Some more directives 3.3 Execution environments 3.4 Sub-workflows 3.5 Nextflow tower 3.6 Advanced challenge", " 3 A more advanced example 3.1 Some more operators 3.2 Some more directives 3.3 Execution environments 3.4 Sub-workflows 3.5 Nextflow tower 3.6 Advanced challenge "],["additional-resources.html", "4 Additional resources", " 4 Additional resources Nextflow documentation: https://www.nextflow.io/docs/latest/index.html Nextflow workshop from the Nextflow authors: https://www.nextflow.io/blog/2022/learn-nextflow-in-2022.html YouTube playlist: https://www.youtube.com/playlist?list=PLPZ8WHdZGxmUv4W8ZRlmstkZwhb_fencI Post on Medium on Nextflow from 23andMe: https://medium.com/23andme-engineering/introduction-to-nextflow-4d0e3b6768d1 Nextflow course on SofwareCarpentries: https://carpentries-incubator.github.io/workflows-nextflow/index.html Other useful links: https://www.nextflow.io/blog/2019/demystifying-nextflow-resume.html "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
